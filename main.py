import os
import feedparser
import requests
from bs4 import BeautifulSoup
import anthropic
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from datetime import datetime, timedelta
import time
import re

# Khá»Ÿi táº¡o Claude client
client = anthropic.Anthropic(
    api_key=os.environ.get('CLAUDE_API_KEY')
)

# Cáº¥u hÃ¬nh RSS feeds - CHá»ˆ VNEXPRESS
RSS_FEEDS = {
    'VnExpress GÃ³c NhÃ¬n': 'https://vnexpress.net/rss/goc-nhin.rss'
}

def clean_html(html_content):
    """LÃ m sáº¡ch HTML content"""
    if not html_content:
        return ""
    
    # Parse HTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # XÃ³a cÃ¡c tháº» khÃ´ng cáº§n thiáº¿t
    for element in soup(['script', 'style', 'img', 'a', 'br', 'div', 'span']):
        element.decompose()
    
    # Láº¥y text vÃ  lÃ m sáº¡ch
    text = soup.get_text()
    
    # LÃ m sáº¡ch whitespace
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def get_article_content_from_rss(entry):
    """Láº¥y content tá»« RSS entry thay vÃ¬ crawl website"""
    print(f"   ğŸ“¡ [DEBUG] Extracting content from RSS entry...")
    
    content_parts = []
    
    # 1. Láº¥y summary tá»« RSS (thÆ°á»ng cÃ³ sáºµn trong VnExpress RSS)
    if hasattr(entry, 'summary') and entry.summary:
        summary_text = clean_html(entry.summary)
        if summary_text and len(summary_text) > 20:
            content_parts.append(summary_text)
            print(f"   âœ… [DEBUG] RSS summary: {len(summary_text)} kÃ½ tá»±")
            print(f"   ğŸ“ [DEBUG] Summary preview: {summary_text[:100]}...")
    
    # 2. Láº¥y description náº¿u cÃ³
    if hasattr(entry, 'description') and entry.description:
        desc_text = clean_html(entry.description)
        if desc_text and len(desc_text) > 20 and desc_text not in content_parts:
            content_parts.append(desc_text)
            print(f"   âœ… [DEBUG] RSS description: {len(desc_text)} kÃ½ tá»±")
    
    # 3. Láº¥y content tá»« RSS content fields
    if hasattr(entry, 'content') and entry.content:
        for content_item in entry.content:
            if hasattr(content_item, 'value') and content_item.value:
                content_text = clean_html(content_item.value)
                if content_text and len(content_text) > 20:
                    content_parts.append(content_text)
                    print(f"   âœ… [DEBUG] RSS content: {len(content_text)} kÃ½ tá»±")
                    break
    
    # 4. Fallback: sá»­ dá»¥ng title extended náº¿u cÃ³
    if not content_parts and hasattr(entry, 'title_detail') and entry.title_detail:
        title_content = clean_html(str(entry.title_detail))
        if title_content and len(title_content) > 20:
            content_parts.append(title_content)
            print(f"   ğŸ†˜ [DEBUG] Using title detail as fallback")
    
    # 5. Final fallback: táº¡o content tá»« title
    if not content_parts:
        print(f"   âš ï¸  [DEBUG] No RSS content found, using title only")
        return f"BÃ i viáº¿t: {entry.title}. KhÃ´ng thá»ƒ láº¥y ná»™i dung chi tiáº¿t tá»« RSS feed."
    
    # GhÃ©p táº¥t cáº£ content láº¡i
    full_content = '\n\n'.join(content_parts)
    
    print(f"   ğŸ“Š [DEBUG] RSS EXTRACTION RESULT:")
    print(f"   ğŸ“ [DEBUG] - Total content parts: {len(content_parts)}")
    print(f"   ğŸ“ [DEBUG] - Full content length: {len(full_content)} kÃ½ tá»±")
    print(f"   ğŸ“‹ [DEBUG] - Content preview: {full_content[:200]}...")
    
    # Giá»›i háº¡n Ä‘á»™ dÃ i
    return full_content[:3000]

def get_article_text_fallback(url):
    """Fallback method - váº«n thá»­ crawl nhÆ°ng khÃ´ng fail náº¿u bá»‹ block"""
    print(f"   ğŸŒ [DEBUG] Fallback crawl attempt: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.8,en;q=0.5',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        print(f"   ğŸ“Š [DEBUG] Fallback response: {response.status_code}")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Thá»­ láº¥y content
            content_parts = []
            
            # VnExpress selectors
            for selector in ['p.description', 'p.Normal', '.fck_detail p', 'article p']:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text().strip()
                    if text and len(text) > 30:
                        content_parts.append(text)
                        if len(content_parts) >= 3:  # Äá»§ 3 Ä‘oáº¡n lÃ  OK
                            break
                if content_parts:
                    break
            
            if content_parts:
                result = '\n\n'.join(content_parts[:5])  # Láº¥y 5 Ä‘oáº¡n Ä‘áº§u
                print(f"   âœ… [DEBUG] Fallback crawl success: {len(result)} kÃ½ tá»±")
                return result[:3000]
        
        print(f"   âŒ [DEBUG] Fallback crawl failed: status {response.status_code}")
        return None
        
    except Exception as e:
        print(f"   âŒ [DEBUG] Fallback crawl exception: {e}")
        return None

def summarize_with_claude(title, content, source):
    """Táº¡o tÃ³m táº¯t báº±ng Claude - Vá»šI RSS CONTENT"""
    print(f"   ğŸ¤– [DEBUG] Báº¯t Ä‘áº§u gá»i Claude API...")
    print(f"   ğŸ“„ [DEBUG] Input content length: {len(content)} kÃ½ tá»±")
    print(f"   ğŸ“‹ [DEBUG] Content preview: {content[:150]}...")
    
    if not content or len(content.strip()) < 30:
        print(f"   âŒ [DEBUG] Content quÃ¡ ngáº¯n: {len(content)} kÃ½ tá»±")
        return f"KhÃ´ng thá»ƒ láº¥y ná»™i dung chi tiáº¿t: {title}"
    
    prompt = f"""HÃ£y viáº¿t tÃ³m táº¯t bÃ i bÃ¡o theo yÃªu cáº§u sau báº±ng tiáº¿ng Viá»‡t:

QUAN TRá»ŒNG:
- Báº¯t Ä‘áº§u ngay báº±ng ná»™i dung chÃ­nh, KHÃ”NG viáº¿t "TÃ³m táº¯t bÃ i viáº¿t" hay dÃ²ng má»Ÿ Ä‘áº§u
- Táº­p trung vÃ o quan Ä‘iá»ƒm vÃ  láº­p luáº­n cá»§a tÃ¡c giáº£
- TrÃ¬nh bÃ y rÃµ rÃ ng, dá»… hiá»ƒu
- KhÃ´ng quÃ¡ 200 tá»«
- KhÃ´ng láº·p láº¡i tiÃªu Ä‘á»

TiÃªu Ä‘á»: {title}

Ná»™i dung bÃ i viáº¿t: {content}

TÃ³m táº¯t:"""

    try:
        print(f"   ğŸ“¡ [DEBUG] Gá»­i request tá»›i Claude API...")
        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=500,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        summary = message.content[0].text.strip()
        print(f"   âœ… [DEBUG] Claude API success!")
        print(f"   ğŸ“ [DEBUG] Summary length: {len(summary)} kÃ½ tá»±")
        print(f"   ğŸ“‹ [DEBUG] Summary preview: {summary[:100]}...")
        return summary
        
    except Exception as e:
        print(f"   ğŸ’¥ [DEBUG] Claude API error: {e}")
        return f"Lá»—i táº¡o tÃ³m táº¯t cho '{title}': {str(e)}"

def parse_rss_feed(url, source_name):
    """Par
